import CodeBlock from "@theme/CodeBlock";

# Unstructured

이 페이지는 [Unstructured](https://unstructured.io)를 LangChain에서 사용하는 방법을 다룹니다.

## What is Unstructured?

Unstructured 는 오픈소스 파이썬 패키지로 문서에서 텍스트를 추출하여 머신러닝 애플리케이션에서 사용할 수 있습니다. 현재 Unstructured 는 워드 문서 (`.doc` 또는 `.docx` 형식), 파워포인트 (`.ppt` 또는 `.pptx` 형식), PDF, HTML 파일, 이미지, 이메일 (`.eml` 또는 `.msg` 형식), epub, 마크다운 및 일반 텍스트 파일을 지원합니다.

`Unstrcutured` 는 파이썬 패키지로 TS/JS 에서는 직접적으로 사용할 수는 없습니다. 하지만 Unstructured 는 다른 프로그래밍 언어로의 전처리 파이프 라인을 지원하기 위해 [REST API](https://github.com/Unstructured-IO/unstrcutured-api) 를 제공합니다. 호스팅 되어 있는 Unstructured API Endpoint 는 `https://api.unstructured.io/general/v0/general` 입니다. 이 서비스를 [이곳](https://github.com/Unstructured-IO/unstructured-api#dizzy-instructions-for-using-the-docker-image)에서 명령어를 찾아서 로컬에 실행할 수도 있습니다.

## Quick start

LangChain 에서 Unstructured 를 아래의 코드와 같이 사용할 수 있습니다.
처리하고 싶은 파일로 파일이름을 바꿔주세요.
만약 컨테이너를 로컬에 뛰우고 싶다면, url 을 `http://127.0.0.1:8000/general/v0/general` 로 바꿔주세요.
더 많은 정보를 보고 싶다면, 이 API [문서 페이지](https://api.unstructured.io/general/docs)를 확인해주세요.

import SingleExample from "@examples/document_loaders/unstructured.ts";

<CodeBlock language="typescript">{SingleExample}</CodeBlock>

## Directories

폴더안에 있는 모든 파일을 불러오기 위해 [`DirectoryLoader`](../modules/indexes/document_loaders/examples/file_loaders/directory.md) 를 상속하고 있는 `UnstructuredDirectoryLoader` 을 사용할 수 있습니다.

import DirectoryExample from "@examples/document_loaders/unstructured_directory.ts";

<CodeBlock language="typescript">{DirectoryExample}</CodeBlock>

현재, `UnstructuredLoader` 는 다음의 문서 형식을 지원합니다:

- 기본 텍스트 파일(`.txt`/`.text`)
- PDFs (`.pdf`)
- Word 문서 (`.doc`/`.docx`)
- 파워포인트 (`.ppt`/`.pptx`)
- 사진  (`.jpg`/`.jpeg`)
- 이메일 (`.eml`/`.msg`)
- HTML (`.html`)
- 마크다운 파일 (`.md`)

`UnstructuredLoader` 의 출력은 다음과 같은 `Document` 객체의 배열입니다.

```typescript
[
  Document {
    pageContent: `Decoder: The decoder is also composed of a stack of N = 6
  identical layers. In addition to the two sub-layers in each encoder layer, the decoder inserts a
  third sub-layer, wh
  ich performs multi-head attention over the output of the encoder stack. Similar to the encoder, we
  employ residual connections around each of the sub-layers, followed by layer normalization. We also
  modify the self
  -attention sub-layer in the decoder stack to prevent positions from attending to subsequent
  positions. This masking, combined with fact that the output embeddings are offset by one position,
  ensures that the predic
  tions for position i can depend only on the known outputs at positions less than i.`,
    metadata: {
      page_number: 3,
      filename: '1706.03762.pdf',
      category: 'NarrativeText'
    }
  },
  Document {
    pageContent: '3.2 Attention',
    metadata: { page_number: 3, filename: '1706.03762.pdf', category: 'Title'
  }
]
```
